{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "import os\n",
    "from nltk import tokenize\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "rouge = Rouge()\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from summa.summarizer import summarize\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentences):\n",
    "\n",
    "    #Load Google pre-trained words \n",
    "    embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        'C:/Users/amitp/Downloads/extractive-document-summarization-master/word2vec/GoogleNews-vectors-negative300.bin.gz', \n",
    "        binary=True, \n",
    "        limit=50000)\n",
    "    word_vectors = embedding_model.wv\n",
    "    max_sen_len = 250\n",
    "    #tokenize sentences\n",
    "    tokenizer = Tokenizer(num_words=30000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences) # replace words with its wordid\n",
    "    padded_sequences = pad_sequences(sequences,maxlen=max_sen_len)\n",
    "    word_index = tokenizer.word_index\n",
    "    # word embedding with 300 dimensions\n",
    "    embedding_weights = {key: embedding_model[word] if word in word_vectors.vocab else\n",
    "                                  np.random.uniform(-0.25, 0.25, word_vectors.vector_size)\n",
    "                            for word, key in word_index.items()}\n",
    "    embedding_weights[0] = np.zeros(word_vectors.vector_size)\n",
    "    #Build a 3D array: 1D fnumber of sentences, 1D for the no of words and 1D for word embedding. \n",
    "    embedded_sentences = np.stack([np.stack([embedding_weights[t] for t in s]) for s in padded_sequences])\n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_testing(Id,text,sumry):\n",
    "    test_data = []\n",
    "    for i,j in enumerate(Id):\n",
    "        TES = np.ones((len(text[i]), 3), dtype=object) \n",
    "        TES[:,0] = \"dummy\"\n",
    "        TES[:,1] = text[i]\n",
    "        embedding = sentence_embedding(text[i])\n",
    "        #embedding = embedding[0::2]\n",
    "        test_data.append((np.array(text[i]), np.array(embedding), np.array(sumry[i])))\n",
    "        print(\"Finished\", i, \"of\", len(Id)+1,\"sentences --\", i/(len(Id)+1),\"%\", end='\\r')\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(test_data):\n",
    "    model_summary = []\n",
    "    for d,doc in enumerate(test_data): \n",
    "\n",
    "        sentences = doc[0]  # sentences in document\n",
    "        x_test_old = doc[1] # embedding of doc\n",
    "        s1 = x_test_old.shape[0] # number of sentences in document\n",
    "        (s3,s4) = x_test_old[0].shape # sentence length,embedding dimension = 300\n",
    "\n",
    "        # Data reshaping for fitting it to model.\n",
    "\n",
    "        x_test = np.random.rand(s1,250,s4,1) # random initialization of tenst tensor\n",
    "        try:\n",
    "            for i in range(s1) :\n",
    "                s = np.array( [ np.pad(x_test_old[i],((250-s3,0),(0,0)),mode='constant') ] )[0] # pad sentences to 250 len\n",
    "                s = np.expand_dims(s, 2)\n",
    "                x_test[i] = s\n",
    "\n",
    "            predicted_scores = model1.predict(x_test, batch_size=256)\n",
    "            argsorted_scores = np.argpartition(np.transpose(predicted_scores)[0], 1)\n",
    "\n",
    "            true_summary = doc[2]\n",
    "            predicted_summary = []\n",
    "            summary_length = 0\n",
    "\n",
    "            i = 0\n",
    "            while i < len(sentences) and summary_length < 250: \n",
    "                sentence = sentences[argsorted_scores[i]]\n",
    "                #if ( dummy_rouge( sentence , predicted_summary ) < threshold ):\n",
    "                sentence = np.array([sentence])\n",
    "                #print(sentence, predicted_summary)\n",
    "                predicted_summary.append(sentence)\n",
    "                summary_length += len(nltk.word_tokenize(sentence[0]))\n",
    "                i+=1\n",
    "            model_summary.append(predicted_summary)\n",
    "        except:\n",
    "            print(\" not possible for doc number \"+str(d)+\" because number of sentences in the document greater than 250\")\n",
    "            model_summary.append(\" \")\n",
    "    return model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# text cleaning = stop word removal,special characters removal , POS - Tagging   for inference logic\n",
    "\n",
    "def getWordnetPos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:#We are igonring everything else other than four of the above \n",
    "         # tags. You can add more if you like\n",
    "        return None \n",
    "#Custom function for toeknization\n",
    "def myTokenizer(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas=[]\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        #nltk return the tag from Penntreebank tagsets\n",
    "        sentTag=nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "        sentTag=[(i.strip(),j) for i,j in sentTag if i.isalpha() and len(i)>2]\n",
    "        #print (sentTag)\n",
    "        for word, tag in sentTag:\n",
    "            # the problem wordnet lemmatizer is that, it recognizes only\n",
    "            # wordnet tags and not the PennTreebank tags. So we shall\n",
    "            # first convert Penntreebank tags to Wordnet tags\n",
    "            wordNetTag=getWordnetPos(tag)\n",
    "            if wordNetTag is None:\n",
    "                continue\n",
    "            else:\n",
    "                lemmas.append(lemmatizer.lemmatize(word,wordNetTag))\n",
    "                \n",
    "    return  lemmas\n",
    "\n",
    "\n",
    "stopWords=nltk.corpus.stopwords.words('english')\n",
    "stopWords+=[\"''\", \"'s\", \"...\", \"``\",\"--\",\"*\",\"-\"]\n",
    "stopWords+=list(string.punctuation)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find similar documents serial numbers which are related to querywords\n",
    "\n",
    "def query_and_op(query_string,number_of_doc, tf_idf):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    \n",
    "    - query_string - input all query words in a single string.\n",
    "    - number_of_doc - number of similar documents to extract\n",
    "    - tf_idf - tf_idf scores saved in a dataframe format where df indexes are vocabulary and columns are docs and cell \n",
    "               values are the tf-idf scores\n",
    "    \n",
    "    output: index of revelent documents or document number best describe the query string words.\n",
    "    \n",
    "    \"\"\"\n",
    "    query = query_string\n",
    "    query_tokens = myTokenizer(query)\n",
    "    \n",
    "    filtered = [] # term-doc\n",
    "    for i in query_tokens:\n",
    "        #print(i)\n",
    "        i = i.lower().strip()\n",
    "        try:\n",
    "            i = tf_doc_df.loc[i].values\n",
    "            filtered.append(list(i))\n",
    "\n",
    "        except:\n",
    "            filtered.append([0.0]*tf_doc_df.shape[0])\n",
    "            \n",
    "    doc_term = np.array(filtered).T                   #doc*term\n",
    "    #doc_term.shape                                   #(number of doc * number of term)\n",
    "    tf_idf_score = np.sum(doc_term,axis=1)\n",
    "    most_revelent_doc = np.argsort(-tf_idf_score)[0]  # sort document according to highest similarity score.\n",
    "    \n",
    "    doc_similarity = linear_kernel(doc_term[most_revelent_doc:most_revelent_doc+1],doc_term)\n",
    "    simlar_doc_query_object = np.argsort(-doc_similarity[0])\n",
    "    test_doc = simlar_doc_query_object[:number_of_doc] # top 20 documents describing the query words\n",
    "    \n",
    "    return test_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract summary:\n",
    "\n",
    "def extract_summary(doc_to_query,similarity_indx, title = None,content = None,summary = None,test_rank = None):\n",
    "    test = doc_to_query.loc[test_doc]\n",
    "    test = test.reset_index()\n",
    "    test = test.drop('index',axis = 1)\n",
    "    test = test.reset_index()\n",
    "    test = test.drop('index',axis = 1)\n",
    "    test = test.reset_index()\n",
    "    \n",
    "    test_data = preprocess_data_for_testing(test.index,test.content,test.Summary_regulation)\n",
    "    prediction = fit_predict(test_data)\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        if title:\n",
    "            print(\"Title :\",query_documents.loc[similarity_indx[i]].longtitle,\"\\n\")\n",
    "        if content:\n",
    "            print(\"Content :\",query_documents.loc[similarity_indx[i]].content,\"\\n\")\n",
    "        if summary:\n",
    "            print(\"summary :\",prediction[i][0][0])\n",
    "        if test_rank:\n",
    "            print(\"test_rank :\",query_documents.loc[similarity_indx[i]].Summary_regulation,\"\\n\")\n",
    "            \n",
    "        print(\"\\n**************************\\n\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 248, 1, 50)        45050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 1, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 603       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 45,657\n",
      "Trainable params: 45,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = tf.keras.models.load_model('best_model_cnn.h5')\n",
    "model1.compile(loss='binary_crossentropy',optimizer=Adadelta(),metrics=['mae'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use tf-idf to extract documents and summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_documents = pd.read_csv(\"C:/Users/amitp/OneDrive/Ryerson-DS/MRP/regulation.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indx =[]\n",
    "content = []\n",
    "for i,j in enumerate(query_documents.content):\n",
    "    j = nltk.sent_tokenize(j)\n",
    "    if \"Interpretation.\" in j:\n",
    "        ind = j.index(\"Interpretation.\")\n",
    "        indx.append(i)\n",
    "        content.append(j[ind+1:])\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_documents = query_documents.loc[indx]\n",
    "query_documents[\"content\"]= content\n",
    "query_documents = query_documents.reset_index(drop= True)\n",
    "query_documents = query_documents.drop(['consolidationyear','registrationyear','regnalyears', 'shorttitle', 'xrefinternal', 'xrefxternal'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating summary by page- rank as we do not have it for regulation data.\n",
    "# converting string summary into list of sentences\n",
    "Summary_regulation = []\n",
    "\n",
    "for i in query_documents.content:\n",
    "    s  = summarize(\" \".join(i), ratio=0.2)\n",
    "    Summary_regulation.append(s)\n",
    "cont = []    \n",
    "\n",
    "for i in Summary_regulation:\n",
    "    i = nltk.sent_tokenize(i)\n",
    "    cont.append(i)\n",
    "query_documents['Summary_regulation']= cont\n",
    "del Summary_regulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create term-document matrix of text corpus: if not already created uncomment this section and genetate these files\n",
    "\n",
    "\n",
    "#documents = pd.read_csv(\"C:/Users/amitp/OneDrive/Ryerson-DS/MRP/regulation.csv\",encoding='utf-8')\n",
    "#documents = documents.loc[indx]\n",
    "#text_corpus = documents.content.values\n",
    "\n",
    "#vectorizer = TfidfVectorizer(max_features=5000, max_df=1.0,tokenizer=myTokenizer, stop_words=stopWords)\n",
    "\n",
    "#tf_doc = vectorizer.fit_transform(text_corpus)\n",
    "#tf_idf_features = vectorizer.get_feature_names()\n",
    "\n",
    "#pickle.dump(tf_doc, open(\"tf_idf_vectorizer.pickle\", \"wb\"))\n",
    "#pickle.dump(tf_idf_features, open(\"tf_idf_features.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### steps to initiate   document searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import if-idf scores . If not already processed then uncomment the above cell get the fils.\n",
    "\n",
    "vectorizer = pickle.load(open(\"tf_idf_vectorizer.pickle\", \"rb\"))\n",
    "features = pickle.load(open(\"tf_idf_features.pickle\", \"rb\"))\n",
    "tf_doc_df = pd.DataFrame(vectorizer.toarray().T,index = features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amitp\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Regulations Respecting Reservations for Airport Apron and Terminal Facilities for Passenger Charter Flights \n",
      "\n",
      "summary : In these Regulations, air carrier  means any person who operates a commercial air service; ( transporteur aérien ) airport manager  means the person in charge of an airport or the authorized representative of that person; ( directeur d’aéroport ) applicant  means an air carrier who applies for a reservation referred to in section 4; ( requérant ) regular charter flight  means a passenger charter flight that is operated by an air carrier on a program basis and for which schedules are provided six months in advance in respect of international flights and three months in advance in respect of domestic flights to the airport manager of each airport listed in the schedule that the air carrier uses for the purposes of the flight, and the apron and terminal facilities of each airport are reserved through established scheduling procedures; ( vol d’affrètement régulier ) reservation  means a reservation of the apron and terminal facilities of an airport listed in the schedule.\n",
      "\n",
      "**************************\n",
      "\n",
      "Title : Regulations Respecting the Disposal of Personal Property Left at Airports \n",
      "\n",
      "summary : In these Regulations, abandoned vehicle  means a vehicle, other than a derelict vehicle, that has been abandoned at an airport or otherwise remains unclaimed at an airport for a period of not less than 30 days; ( véhicule abandonné ) airport  means an airport or aerodrome under the administration and control of the Minister of Transport; ( aéroport ) Airport Manager  means the Department of Transport official in charge of the airport or his duly authorized representative; ( directeur ) Department  means the Department of Transport; ( ministère ) derelict vehicle  means a vehicle, other than an abandoned vehicle, that has been abandoned at an airport or otherwise remains unclaimed at an airport for a period of not less than 14 days, and has a market value less than $200; ( épave ) owner , with respect to a motor vehicle, means a person who holds legal title to it or a person in whose name it is registered or is required to be registered by the laws of a province and includes a conditional purchaser, lessee or mortgagor who is entitled to or is in possession of the motor vehicle; ( propriétaire ) personal property  means all property, other than a vehicle, not owned by Her Majesty; ( biens personnels ) Regional Administrator  means the Regional Administrator, Canadian Air Transportation Administration, who has jurisdiction over the airport in question; ( administrateur régional ) vehicle  means a self-powered device in, on or by which a person or property is or may be transported or drawn upon a road, except a device used exclusively on stationary rails or tracks.\n",
      "\n",
      "**************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#   give the input words to query stings which will extract the documents numbers.\n",
    "query_string =  \"airport security\"\n",
    "number_of_doc = 2\n",
    "tf_idf = tf_doc_df\n",
    "test_doc = query_and_op( query_string, number_of_doc, tf_doc_df )\n",
    "# run the function extract summary as per required setting to get final output\n",
    "doc_to_query = query_documents\n",
    "similarity_indx = test_doc\n",
    "title = True\n",
    "content = False\n",
    "summary = True\n",
    "test_rank = False\n",
    "extract_summary(doc_to_query,similarity_indx, title,content,summary,test_rank)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
